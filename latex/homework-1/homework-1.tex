\documentclass[a4paper,12pt]{article}

\input{../common/common.tex}


\begin{document}



\title{Homework 1 \\ NUMA11}
\author{
  Karl \textsc{Lind\'{e}n} \\
  <karl.linden.887@student.lu.se> \\
  Oscar \textsc{Nilsson} \\
  <erik-oscar-nilsson@live.se>
}

\maketitle
\thispagestyle{empty}

\newpage


\subsection*{Task 1}

Let \(\lambda\) be the largest eigenvalue of \(A\) and let \(v\) be an
eigenvector corresponding to \(\lambda\).
The result follows from
\[
  \rho(A)
    = |\lambda|
    = \frac{|\lambda|\|v\|}{\|v\|}
    = \frac{\|\lambda v\|}{\|v\|}
    = \frac{\|Av\|}{\|v\|}
    \le \sup_{x \ne 0} \frac{\|Ax\|}{\|x\|}
    = \|A\|.
\]


\subsection*{Task 2}

If \(A\) is diagonalizable we can use the fact that all eigenvalues are less
than \(1\) and use
\[ \lim_{n \to \infty} A^n = \lim_{n \to \infty} XD^nX^{-1} = 0, \]
where the last follows from the fact that
\[
  D^n
    = \diag(\lambda_1^n, \lambda_2^n, \dots, \lambda_m^n)
    \to \diag(0, 0, \dots, 0)
    = 0
\]
as \(n \to \infty\).

We now laborously show the result for the general case.
Because \(\|\cdot\|\) is an operator norm we have
\[ \|AB\| \le \|A\|\|B\|. \]
Induction gives
\[ \|A^n\| \le \|A\|^n, \]
so
\[ \lim_{n \to \infty} \|A^n\| = 0. \]

The result now follows from the next proposition.

\begin{proposition}
  Let \((A_n)_{n=1}^\infty\) be a sequence of \(m \times m\) matrices and
  \(\|\cdot\|\) an operator norm.
  Then
  \[ \lim_{n \to \infty} \|A_n\| = 0 \implies \lim_{n \to \infty} A_n = 0. \]
\end{proposition}
\begin{proof}
  Suppose toward a contradiction that \(A_n \not \to 0\) as \(n \to \infty\).
  Denote by \(a_{ij}^{(n)}\) the entry on row \(i\) and column \(j\) of \(A_n\).
  Then for some \(i\) and \(j\) there exists an \(\varepsilon > 0\) such that
  regardless of \(N\) there is an \(n \ge N\) such that
  \(|a_{ij}^{(n)}| \ge \varepsilon\).
  By the equivalence of norms, we have that \(\|x\| \ge c\|x\|_\infty\) for all
  \(x \in \mathbb{R}^m\) and some \(c > 0\).
  Now we have
  \begin{align*}
    \|A_n e_j\|
      &= \|(a^{(n)}_{1j}, a^{(n)}_{2j}, \dots, a^{(n)}_{mj})\| \\
      &\ge c\|(a^{(n)}_{1j}, a^{(n)}_{2j}, \dots, a^{(n)}_{mj})\|_\infty \\
      &= c \max_{1 \le k \le m} |a_{kj}^{(n)}| \\
      &\ge c |a^{(n)}_{ij}| \\
      &\ge c \varepsilon,
  \end{align*}
  but this shows that for all \(N\) there is \(n \ge N\) such that
  \[
    \|A_n\|
      \ge \frac{\|A_n e_j\|}{\|e_j\|}
      \ge d\epsilon
  \]
  where \(d = c/\|e_j\|\).
  This is a contradiction, since \(\|A_n\| \to 0\) as \(n \to \infty\).
  Therefore the assumption that \(A_n \not \to 0\) is false, completing the
  proof.
\end{proof}


\subsection*{Task 3}

In this task, \(x = (x_1,x_2,\dots,x_n)\) and
\(j = \argmax_{1 \le i \le n} |x_i|\).
Then
\[ \|x\|_\infty = \max_{1 \le i \le n} |x_i| = |x_j|. \]

TBD: Exercises 3 and 4 are wrong since the maximum operates on different
sets.

\begin{enumerate}
  \item
    The inequality follows from
    \[
      |x_j|
        = \sqrt{x_j^2}
        \le \left(\sum_{i=1}^n x_i^2\right)^{1/2}
        = \|x\|_2.
    \]

  \item
    This follows from
    \[
      \|x\|_2
        = \left(\sum_{i=1}^n x_i^2\right)^{1/2}
        \le \left(\sum_{i=1}^n x_j^2\right)^{1/2}
        = \sqrt{nx_j^2}
        = \sqrt{n}|x_j|
        = \sqrt{n}\|x\|_\infty.
    \]

  \item
  The inequality in (2) can be rewritten to $ \frac{1}{\sqrt{n}} \|x\|_2 \le \|x\|_\infty$, this do we use to prove the following,
  \begin{equation}
  \|A\|_\infty \underset{x\not=0}{\sup} \cfrac{\|Ax\|_2}{\|x\|_\infty }\le \underset{x\not=0}{\sup} \cfrac{\|Ax\|_2}{\frac{1}{\sqrt{n}}\|x\|_2}=\sqrt{n}\|A\|_2.
  \end{equation}
  
  
  \iffalse
    The maximum preserves inequalities, so by 1 we have:
    \[
      \|A\|_\infty
        = \max_{\|x\| = 1} \|Ax\|_\infty
        \le \max_{\|x\| = 1} \|Ax\|_2
        = \|A\|_2,
    \]
    and obviously \(\|A\|_2 \le \sqrt{n}\|A\|_2\).
  \fi
  \item
  
  This can be shown by,
  \begin{equation}
  \|A\|_2=\underset{x\not=0}{\sup} \cfrac{\|Ax\|_2}{\|x\|_2}\le \underset{x\not=0}{\sup} \cfrac{\sqrt{n}\|Ax\|_\infty}{\|x\|_\infty} =\sqrt{n}\|A\|_\infty.
  \end{equation}
  \iffalse
    Again by the fact that the maximum preserves inequalities, 2 gives
    \begin{align*}
      \|A\|_2
      &= \max_{\|x\| = 1} \|Ax\|_2 \\
      &\le \max_{\|x\| = 1} \sqrt{n}\|Ax\|_\infty \\
      &= \sqrt{n} \max_{\|x\| = 1} \|Ax\|_\infty \\
      &= \sqrt{n} \|Ax\|_\infty.
    \end{align*}
  \fi
\end{enumerate}


\subsection*{Task 4}

Let
\[
  a = (a_1, a_2, \dots, a_n)
  \quad \text{and} \quad
  A =
    \begin{bmatrix}
      a_1 & a_2 & \cdots & a_n
    \end{bmatrix}.
\]
By definition we have that
\[ \frac{\|Ax\|_2}{\|x\|_2} \le \|A\|_2, \]
for all \(x\in \mathbb{R}^n\).
Because \(\|r\|_2 = |r|\) for all \(r \in \mathbb{R}\) we have
\[
  \cfrac{\|Aa\|_2}{\|a\|_2}
    = \frac{\|\sum_{i=1}^n a_i^2\|_2}{\sqrt{\sum_{i=1}^n a_i^2}}
    = \left(\sum_{i=1}^{n} a_i^2\right)^{1/2}
    = \|a\|_2,
\]
whence, \(\|a\|_2\le \|A\|_2\).
An application of the Cauchy-Schwarz inequality gives
\[
  \frac{\|Ax\|_2}{\|x\|_2}
    = \frac{\|(a,x)\|_2}{\|x\|_2}
    \le \frac{\|a\|_2 \|x\|_2}{\|x\|_2}
    = \|a\|_2.
\]
Hence, \(\|A\|_2= \|a\|_2\).

The same result hold for the 1-norm.
Firstly
\[
  \|A\|_1
    = \max_{\|x\|_1 = 1} \|Ax\|_1\\
    = \max_{\|x\|_1 = 1} \left| \sum_{i=1}^n a_i x_i\right|
    \le \max_{\|x\|_1 = 1} \sum_{i=1}^n |a_i||x_i|
    \le \max_{1 \le i \le n} |a_i|,
\]
and secondly
\[ \|Ae_i\|_1 = \|a_i\|_1 = |a_i| \le \|A\|_1, \quad 1 \le i \le n. \]
Hence, $ \|A\|_1=\underset{1\le i \le n}{\max} |a_i|$.

Consider
\[
  a = (1, 2)
  \quad \text{and} \quad
  A =
    \begin{bmatrix}
      1 & 2
    \end{bmatrix}.
\]
We have
\[
  \|A\|_\infty
    = \max_{\|x\|_\infty = 1} \|Ax\|_\infty
    \ge \|1 \cdot 1 + 2 \cdot 1\|_\infty
    = 3,
\]
but \(\|a\|_\infty = 2\), showing that \(\|A\|_\infty \ne
\|a\|_\infty\).
Thus, the result is not true of the \(\infty\)-norm.


\subsection*{Task 5}

Let \(\varepsilon > 0\) be given.
We want \(\left|\|x\|_\beta - \|y\|_\beta\right| < \varepsilon\)
whenever \(\|x-y\|_1 < \delta\), for some \(\delta > 0\).
Let
\[ R = \max_{1 \le i \le n} \|e_i\|_\beta. \]
Now
\[
  \|x\|_\beta
    = \left\|\sum_{i=1}^n x_i e_i\right\|_\beta
    \le \sum_{i=1}^n \|x_i e_i\|_\beta
    = \sum_{i=1}^n |x_i| \|e_i\|_\beta
    \le R \sum_{i=1}^n |x_i|
    = R \|x\|_1.
\]
Whenever \(\|x-y\|_1 < \frac{\varepsilon}{R}\), we have by the reversed
triangle inequality that
\[
  \left|\|x\|_\beta - \|y\|_\beta\right|
    \le \|x - y\|_\beta
    \le R \|x - y\|_1
    < \varepsilon,
\]
so \(\delta = \varepsilon/R\) suffices.


\subsection*{Task 6}

Let \(A = uv^T\)
If either \(u = 0\) or \(v = 0\) then certainly \(\rank(A) = 0\).
Therefore let \(u \ne 0\) and \(v \ne 0\).
Then \(u\) and \(v\) have some non-zero entries \(u_i\) and \(v_j\), but this
implies that \(u_i v_j\), which is the \(i,j\):th entry of \(A\), is non-zero,
showing that \(A\) has rank at least \(1\).
Any two columns \(v_j u\) and \(v_k u\) of \(A\) must be linearly dependent
because they are parallel to each other.
Therefore \(\rank(A) \le 1\) and it follows that \(\rank(A) = 1\).


\subsection*{Task 7}

Let \(u\) be an \(m\) vector and \(v\) an \(n\) vector.
Define \(k = \min(m,n)\).
Set \(u_1 = u/\|u\|\) and \(v_1 = v/\|v\|\).
Extend \(u_1\) to a basis \(u_1, u_2, \dots, u_m\) for \(\mathbb{R}^m\) and
extend \(v_1\) to a basis \(v_1, v_2, \dots, v_k\) for \(\mathbb{R}^k\).
The reduced SVD is given by \(uv^T = \hat{U} \hat{\Sigma} V^T\), where
\[
  \hat{U} =
    \begin{bmatrix}
      u_1 & \cdots & u_k
    \end{bmatrix}, \quad
  \hat{\Sigma} = \diag(\|u\|\|v\|, 0, \dots, 0),\quad
  V =
    \begin{bmatrix}
      v_1 & \cdots & v_k
    \end{bmatrix}.
\]
Here \(\hat{U}\) is the matrix with \(u_1,\dots,u_k\) as columns,
\(\hat{\Sigma}\) is an \(k \times k\) diagonal matrix and \(V\) is the matrix
with \(v_1,\dots,v_k\) as columns.
The full SVD is given by \(uv^T = U \Sigma V\) where
\[
  U =
    \begin{bmatrix}
      u_1 & \cdots & u_m
    \end{bmatrix},\quad
  \Sigma =
    \begin{bmatrix}
      \hat{\Sigma} \\
      0
    \end{bmatrix}.
\]
Here 0 is the \(m-k \times n\) matrix containing only zeroes.

Below is a numeric example in python.
The output of this example follows right after.
Note that the output is as expected, but with a different sign on the vectors in
both the \(U\) and \(V\) matrices.

\lstinputlisting{../../python/homework-1/task_7.py}
\VerbatimInput{../../python/homework-1/task_7.out}

\end{document}
