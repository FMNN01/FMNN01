\documentclass[a4paper,12pt]{article}

\input{../common/common.tex}


\begin{document}



\title{Homework 1 \\ NUMA11}
\author{
  Karl \textsc{Lind\'{e}n} \\
  <karl.linden.887@student.lu.se> \\
  Oscar \textsc{Nilsson} \\
  <erik-oscar-nilsson@live.se>
}

\maketitle
\thispagestyle{empty}

\newpage


\subsection*{Task 1}

Let \(\lambda\) be the largest eigenvalue of \(A\) and let \(v\) be an
eigenvector corresponding to \(\lambda\).
The result follows from
\[
  \rho(A)
    = |\lambda|
    = \frac{|\lambda|\|v\|}{\|v\|}
    = \frac{\|\lambda v\|}{\|v\|}
    = \frac{\|Av\|}{\|v\|}
    \le \sup_{x \ne 0} \frac{\|Ax\|}{\|x\|}
    = \|A\|.
\]


\subsection*{Task 2}

If \(A\) is diagonalizable we can use the fact that all eigenvalues are less
than \(1\) and use
\[ \lim_{n \to \infty} A^n = \lim_{n \to \infty} XD^nX^{-1} = 0, \]
where the last follows from the fact that
\[
  D^n
    = \mathrm{diag}(\lambda_1^n, \lambda_2^n, \dots, \lambda_m^n)
    \to \mathrm{diag}(0, 0, \dots, 0)
    = 0
\]
as \(n \to \infty\).

We now laborously show the result for the general case.
Because \(\|\cdot\|\) is an operator norm we have
\[ \|AB\| \le \|A\|\|B\|. \]
Induction gives
\[ \|A^n\| \le \|A\|^n, \]
so
\[ \lim_{n \to \infty} \|A^n\| = 0. \]

The result now follows from the next proposition.

\begin{proposition}
  Let \((A_n)_{n=1}^\infty\) be a sequence of matrices.
  If \(\lim_{n \to \infty} \|A_n\| \to 0\), then
  \(\lim_{n \to \infty} A_n = 0\).
\end{proposition}
\begin{proof}
  Suppose toward a contradiction that \(A_n \not \to 0\) as \(n \to \infty\).
  Denote by \(a_{ij}^{(n)}\) the entry on row \(i\) and column \(j\) of \(A_n\).
  Then for some \(i\) and \(j\) there exists an \(\varepsilon > 0\) such that
  regardless of \(N\) there is an \(n \ge N\) such that
  \(|a_{ij}^{(n)}| \ge \varepsilon\).
  By the equivalence of norms, we have that \(\|x\| \ge c\|x\|_\infty\) for all
  \(x \in \mathbb{R}^m\) and some \(c > 0\).
  Now we have
  \begin{align*}
    \|A_n e_j\|
      &= \|(a^{(n)}_{1j}, a^{(n)}_{2j}, \dots, a^{(n)}_{mj})\| \\
      &\ge c\|(a^{(n)}_{1j}, a^{(n)}_{2j}, \dots, a^{(n)}_{mj})\|_\infty \\
      &= c \max_{1 \le k \le m} |a_{kj}^{(n)}| \\
      &\ge c |a^{(n)}_{ij}| \\
      &\ge c \varepsilon,
  \end{align*}
  but this shows that for all \(N\) there is \(n \ge N\) such that
  \[
    \|A_n\|
      \ge \frac{\|A_n e_j\|}{\|e_j\|}
      \ge d\epsilon
  \]
  where \(d = c/\|e_j\|\).
  This is a contradiction, since \(\|A_n\| \to 0\) as \(n \to \infty\).
  Therefore the assumption that \(A_n \not \to 0\) is false, completing the
  proof.
\end{proof}


\subsection*{Task 3}

In this task, \(x = (x_1,x_2,\dots,x_n)\) and
\(j = \argmax_{1 \le i \le n} |x_i|\).
Then
\[ \|x\|_\infty = \max_{1 \le i \le n} |x_i| = |x_j|. \]

TBD: Exercises 3 and 4 are wrong since the maximum operates on different
sets.

\begin{enumerate}
  \item
    The inequality follows from
    \[
      |x_j|
        = \sqrt{x_j^2}
        \le \left(\sum_{i=1}^n x_i^2\right)^{1/2}
        = \|x\|_2.
    \]

  \item
    This follows from
    \[
      \|x\|_2
        = \left(\sum_{i=1}^n x_i^2\right)^{1/2}
        \le \left(\sum_{i=1}^n x_j^2\right)^{1/2}
        = \sqrt{nx_j^2}
        = \sqrt{n}|x_j|
        = \sqrt{n}\|x\|_\infty.
    \]

  \item
    The maximum preserves inequalities, so by 1 we have:
    \[
      \|A\|_\infty
        = \max_{\|x\| = 1} \|Ax\|_\infty
        \le \max_{\|x\| = 1} \|Ax\|_2
        = \|A\|_2,
    \]
    and obviously \(\|A\|_2 \le \sqrt{n}\|A\|_2\).

  \item
    Again by the fact that the maximum preserves inequalities, 2 gives
    \begin{align*}
      \|A\|_2
      &= \max_{\|x\| = 1} \|Ax\|_2 \\
      &\le \max_{\|x\| = 1} \sqrt{n}\|Ax\|_\infty \\
      &= \sqrt{n} \max_{\|x\| = 1} \|Ax\|_\infty \\
      &= \sqrt{n} \|Ax\|_\infty.
    \end{align*}
\end{enumerate}


\subsection*{Task 4}

Let
\[
  a = (a_1, a_2, \dots, a_n)
  \quad \text{and} \quad
  A =
    \begin{bmatrix}
      a_1 & a_2 & \cdots & a_n
    \end{bmatrix}.
\]
By definition we have that
\[ \frac{\|Ax\|_2}{\|x\|_2} \le \|A\|_2, \]
for all \(x\in \mathbb{R}^n\).
Because \(\|r\|_2 = |r|\) for all \(r \in \mathbb{R}\) we have
\[
  \cfrac{\|Aa\|_2}{\|a\|_2}
    = \frac{\|\sum_{i=1}^n a_i^2\|_2}{\sqrt{\sum_{i=1}^n a_i^2}}
    = \left(\sum_{i=1}^{n} a_i^2\right)^{1/2}
    = \|a\|_2,
\]
whence, \(\|a\|_2\le \|A\|_2\).
An application of the Cauchy-Schwarz inequality gives
\[
  \frac{\|Ax\|_2}{\|x\|_2}
    = \frac{\|(a,x)\|_2}{\|x\|_2}
    \le \frac{\|a\|_2 \|x\|_2}{\|x\|_2}
    = \|a\|_2.
\]
Hence, \(\|A\|_2= \|a\|_2\).

The same result hold for the 1-norm.
Firstly
\[
  \|A\|_1
    = \max_{\|x\|_1 = 1} \|Ax\|_1\\
    = \max_{\|x\|_1 = 1} \left| \sum_{i=1}^n a_i x_i\right|
    \le \max_{\|x\|_1 = 1} \sum_{i=1}^n |a_i||x_i|
    \le \max_{1 \le i \le n} |a_i|,
\]
and secondly
\[ \|Ae_i\|_1 = \|a_i\|_1 = |a_i| \le \|A\|_1, \quad 1 \le i \le n. \]
Hence, $ \|A\|_1=\underset{1\le i \le n}{\max} |a_i|$.

Consider
\[
  a = (1, 2)
  \quad \text{and} \quad
  A =
    \begin{bmatrix}
      1 & 2
    \end{bmatrix}.
\]
We have
\[
  \|A\|_\infty
    = \max_{\|x\|_\infty = 1} \|Ax\|_\infty
    \ge \|1 \cdot 1 + 2 \cdot 1\|_\infty
    = 3,
\]
but \(\|a\|_\infty = 2\), showing that \(\|A\|_\infty \ne
\|a\|_\infty\).
Thus, the result is not true of the \(\infty\)-norm.


\subsection*{Task 5}

Let \(\varepsilon > 0\) be given.
We want \(\left|\|x\|_\beta - \|y\|_\beta\right| < \varepsilon\)
whenever \(\|x-y\|_1 < \delta\), for some \(\delta > 0\).
Let
\[ R = \max_{1 \le i \le n} \|e_i\|_\beta. \]
Now
\[
  \|x\|_\beta
    = \left\|\sum_{i=1}^n x_i e_i\right\|_\beta
    \le \sum_{i=1}^n \|x_i e_i\|_\beta
    = \sum_{i=1}^n |x_i| \|e_i\|_\beta
    \le R \sum_{i=1}^n |x_i|
    = R \|x\|_1.
\]
Whenever \(\|x-y\|_1 < \frac{\varepsilon}{R}\), we have by the reversed
triangle inequality that
\[
  \left|\|x\|_\beta - \|y\|_\beta\right|
    \le \|x - y\|_\beta
    \le R \|x - y\|_1
    < \varepsilon,
\]
so \(\delta = \varepsilon/R\) suffices.


\subsection*{Task 6}

TBD


\subsection*{Task 7}

TBD


\end{document}
